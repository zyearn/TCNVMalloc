
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt, conference, compsocconf]{IEEEtran}
\usepackage{graphicx}
\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% correct bad hyphenation here
% Add the compsocconf option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired

\title{Wamalloc: An Efficient Wear-Aware Allocator for Non-Volatile Memory}

% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

\author{\IEEEauthorblockN{Jiashun Zhu, Sumin Li, Linpeng Huang}
\IEEEauthorblockA{
Department of Computer Science and Engineering\\
Shanghai Jiao Tong University\\
Email: zhu1412043, lisumin\_2013, lphuang@sjtu.edu.cn}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
Non-volatile memory(NVM) promises a DRAM replacement in computer systems due to its attractive characteristics.
However, the low endurance problem limits its practical applications.
In this paper, we propose Wamalloc, an efficient NVM memory allocator to extend the lifetime of NVM in the software level.
An elaborate hybrid wear-leveling policy is proposed in this paper to achieve wear-leveling without hardware overhead.
The evaluations show that
the wear-leveling policy of Wamalloc outperforms that of NVMalloc from 3\% to 30\%,
and the total memory consumption of Wamaloc outperforms that of NVMalloc about 60\% and 10\% under uniform and random workloads.
In addition, the allocation performance of Wamalloc is better than the standard glibc malloc and NVMalloc 
by 98\% and 97\% under uniform workloads,
83\% and 86\% under random workloads.

\end{abstract}

\begin{IEEEkeywords}
non-volatile memory; memory allocator; wear-leveling

\end{IEEEkeywords}

\section{Introduction}

For decades, dynamic random-access memory(DRAM) has been used as the main memory of computer system.
However, with the great demands of performance and energy-constrain applications, the defects of DRAM have become a main concern,
for example, the limited density\cite{mandelman2002challenges}
and its high energy consumption\cite{lefurgy2003energy}. 
Fortunately, an emerging new technology named non-volatile memory(NVM) provides the solution to address these problems,
NVM, such as Phase-change memory(PCM)\cite{huang2009phase},
spin transfer torque(STT-RAM)\cite{apalkov2013spin}, and memristors\cite{ho2009nonvolatile}, 
has the advantages of high density, low power consumption and byte-addressable,
which make it a better alternative to be the main memory when compared with DRAM.

However, NVM suffers from limited write endurance. 
For example, a typical PCM cell permanently fails after nearly $10^7$ to $10^9$ writes. 
In such a circumstance, a poorly designed memory allocator will break the PCM in a short time. 
For purpose of tackling this problem and support a software level wear-aware memory allocator,
a new memory allocator targeted for NVM platform must be designed for applications running on NVM.

There are already some researches on memory allocators for NVM\cite{volos2011mnemosyne,coburn2011nv,moraru2013consistent}.
Unfortunately, none of these allocators can provide both an accurate wear-leveling policy
and a good allocation performance simultaneously.
The reason is that they mainly focused on the whole toolboxes of NVM or the wear-leveling policy is not accurate enough.
As a result, a new NVM allocator that is more accurate in wear-leveling and has a better performance is urgently expected.

%no NVM allocators at the time of writing can 
In this paper, we propose Wamalloc, 
a wear-aware allocator that provide wear-leveling without degrading the performance of allocator 
when compared with the state-of-the-art allocator. 
Specificly, this paper introduces three main techniques of NVM allocator.
First, we propose a novel hybrid(fine-grained and coarse-grained) wear-leveling policy that are proven to be more accurate;
Besides, we use a thread-cache structure and lock optimizations that result in a better performance 
compared with other NVM and DRAM allocators.
At last, Wamalloc decouples metadata and data management,
since putting metadata into DRAM will decrease plenty of writes to NVM, which will probably extend the lifetime of PCM.

Our contributions can be summarized as follows:
\begin{enumerate}
    \item We propose Wamalloc, which
        (1) uses a hybrid wear-leveling policy focusing on an accurate wear-leveling policy;
        (2) has a good performance compared with the state-of-the-art NVM and DRAM allocators;
        (3) decouples metadata and data management to reduce huge small writes.
    \item We implement Wamalloc as an userspace library, which depends on no special requirements on operating system and hardware modifications.
\end{enumerate}

We implement a prototype of Wamalloc and 
evaluate its performance and some critical aspects against several memory allocators.
Experiment shows that under both uniform and random workloads, 
Wamalloc outperforms the prior allocators in aspect of average block allocation frequency, total memory consumption and allocation latency.

%To our knowledge, Wamalloc ...

\section{Background and Assumptions}

In this section, we briefly introduce the non-volatile memory and
make some assumptions about operating system support for these memories.

\subsection{Non-Volatile Memory}

These new emerging memory technologies hope to fill the gap between main memory and external storage.
They are fast, persistent and byte addressable, which make them the best candidates for DRAM.
Three typical technologies are phase change memory, memristor and spin transfer torque.

In order to get a more perceptual view, we will take PCM as an illustration. 
PCM is a non-volatile memory that exploits the unique behavior of chalcogenide glass.
It stores bits by heating a nanoscale piece of chalcogenide glass and makes it change state by injecting current through memory cell.
Specificly, there are two states.
One is called a crystalline state representing state (1), and the other is called an amorphous state representing state (0).

Besides PCM, the other two alternative technologies(memristor and spin transfer torque) are also completing to be candidates for DRAM.
Although These technologies have good characteristics to be the main memory,
they suffer from different kinds of weakness, such as endurance and asymmetry of read and write latency. 
For example, memristor may wear-out after $10^8$ writes, while DRAM has an unlimited endurance.
Table~\ref{tab:NVMvsDRAM} presents the different characteristics of different NVM and DRAM.

\begin{table}[h]
    \centering
    \caption{ {\upshape Characteristic of NVM and DRAM}}
    \begin{tabular}{|c|c|c|c|} \hline
        & Read & Write & Endurance\\ \hline
        DRAM & 60ns & 60ns & nearly unlimited\\ \hline
        PCM & 50-85ns & 150ns-1us & $10^8$-$10^{12}$\\ \hline
        Memristor & 100ns & 100ns & $10^8$\\ \hline
        STT-RAM & 6ns & 13ns & $10^{15}$\\ \hline
    \end{tabular}
\label{tab:NVMvsDRAM}
\end{table}

In the following sections, we will collectively refer to these memories as NVM, 
because our focus is emphasized on an efficient wear-leveling policy based on non-volatile memory,
instead of the performance of different kinds of NVM.

\subsection{NVM in memory system}

Due to the limitations(limited endurance and slow writes),
NVM is not likely to replace DRAM as the only main memory in computer system.
We assume that at least for write-intensive applications,
computer systems will use a combination of DRAM and NVM, taking their respective advantages.
The logical view of the hybrid memory system is shown in Figure~\ref{fig:NVMposition}.
The NVM library layer provides the ability for applications to manipulate NVM memory pages.

Wamalloc is proposed to be in the layer of NVM library, aiming at providing an efficient wear-aware NVM allocator.

\begin{figure}[h]
\centering
\includegraphics[width=1.5in]{NVMposition.png}
\caption{Logical view of memory system}
\label{fig:NVMposition}
\end{figure}

\subsection{Operating system support}

We expect that the virtual memory mechanism can map both volatile and non-volatile memory into the process's address space,
which means a page table(e.g.,in x86) or a inverted page table(e.g.,in PowerPC) is involved when a memory address is read or written.
An application can request NVM memory pages from operating system via an extended system call(e.g., mmap).

In order to restore to the state that a process was previously in when process restart happens, 
operating system must be able to allow a process to remap its original NVM memory pages to its virtual address space.
Thus, operating system should provide a mechanism to retrieve NVM memory pages based on some dedicated NVM management systems.
There are several existing systems to handle this problem\cite{coburn2011nv, satyanarayanan1994lightweight, volos2011mnemosyne}.
In this paper, we assume that operating system has some access controls on NVM memory pages, 
which may be similar to that of filesystem.

\section{Design of Wamalloc}

In this section, we propose Wamalloc, 
an memory allocator for NVM with the dedicated design for wear-aware purpose 
without the descend of allocation performance.

\subsection{Design Principles}

The most important design goal of Wamalloc is wear-leveling. 
Without hardware level wear-leveling, every time applications request memory,
a memory block with the least allocation frequency is expected to be allocated.
However, a strict policy that allocates the exact block may introduce extra time penalty.
In the later subsections we will see a hybrid method to tackle this problem.

Another requirement of Wamalloc is low allocation latency,
otherwise allocator would probably become the bottleneck of the entire application,
which results in the allocator to be useless. 
To reduce the latency, every thread should have its own local heap to reduce the possible lock contention,
and all memory owned by threads should be managed by a global heap.
However, managing a global heap makes the usage of lock inevitable.
As we will see later, we use some specific methods to reduce the usage of lock.

\subsection{Overview Structure}

The structure of Wamalloc is showed in Figure~\ref{fig:arch}.
Wamalloc maintains a local heap for each thread, and a global heap for all threads.
The reason why we use such a design is from the obvious observation that if a request from a thread can be satisfied by its local heap,
the allocation latency will be very low since there is no lock contention.
However, a global heap that manage all the memory will inevitably introduce a lock.
One of our implementation focus is on how to reduce the usage of lock.

\begin{figure}
\centering
\includegraphics[width=3in]{arch.png}
\caption{Overall architecture}
\label{fig:arch}
\end{figure}

Each local heap contains several fixed-size memory chunks.
Each memory chunk is further divided into several memory blocks based on its size class.
Each thread maintains several chunks, and Wamalloc treats memory blocks as basic allocation unit.
How the memory blocks are allocated and how the chunks are moved between local heap and global heap
are based on our wear-leveling policy to be described later.

\subsection{Memory Chunk}

Memory chunk is the basic unit that would be transferred between global heap and local heap. 
A memory chunk consists of two parts: a small chunk header and a 64KB chunk body. 
Chunk body is divided into several memory blocks based on its size class.
Obviously, with smaller size class comes more memory blocks. 
Chunk header stores metadata such as the number of remaining free blocks, its size class and so on.

Since a memory chunk body only contains 64KB memory, all the memory requests smaller than 64KB are regarded as small requests,
which are directly handled by local heap without acquiring the lock in the global heap.
Small allocations are classified into several predefined class,
just like TCMalloc\cite{ghemawat2009tcmalloc} and SSMalloc\cite{liu2012ssmalloc} does. 
A memory request is served by a memory chunk with the nearest size class. 
Wamalloc uses two sets of size class: fine-grained(16B, 24B, 32B,..., 256B), coarse-grained(384B, 512B,..., 32768B, 49152B, 65536B). 
For the memory requests larger than 64KB, Wamalloc will redirect the request to operating system via system call such as mmap.

% classes of allocatable objects
\subsubsection{Classes of allocatable blocks}
The chunk overview is shown in Figure~\ref{fig:chunk}.
The usable memory blocks in a memory chunk can be classified into two categories:
first-allocated blocks indicated by a free pointer,
which have never been allocated since the chunk is assigned to its local heap,
and reusable blocks indicated by a free list, which have been allocated and freed back to the local heap.
All the blocks starting from the free pointer or storing at the free list are ready to be allocated to applications. 
How to organize the free list will be discussed in the following wear-leveling policy subsection.

\begin{figure}[h]
\centering
\includegraphics[width=3.2in]{chunk.png}
\caption{Chunk structure}
\label{fig:chunk}
\end{figure}

% how to choose chunk size
\subsubsection{How to choose chunk size}
The size of memory chunk is very critical to the performance of Wamalloc. 
If the chunk size is too small, 
the memory blocks will be used up very soon.
Frequently communicating with the global heap will greatly decrease the performance.
If the chunk size is too large, lots of memory blocks may remain clean or unused, which leads to a huge memory waste. 
A working method used by many other allocators is to choose chunks of small size to serve small blocks, 
and chunks of large size to serve large blocks,
then there will always be available usable memory blocks no matter what size class a chunk belongs to.
The drawbacks are also obvious:
since the memory chunk size is different, it increases the complexity of global heap management.
Consequently this will cause the extra time penalty and the possible memory fragment, 
which may result in more physical memory consumption than an application really needs.

Take above discussions into consideration, 
we use a uniform memory chunk size for all size class. 
In most cases, memory allocations are small size, 
which are naturally guaranteed that there are several usable memory blocks. 
%For a special case, if a memory request is 64KB, there is only one block in the entire chunk body, so the allocation policy just view this block as the last remaining block 
%and return it to the application. 
%After that if the application needs a new 64KB memory, a new chunk is allocated to serve this request.
Further, This design gives us several other benefits: 
\begin{enumerate}
    \item Since all memory chunks are of the same size, an clean chunk can easily be transferred from one size class to another without any time-consuming operations.
    \item The uniform memory chunk greatly simplified the design of the global heap, 
        which results in a lower time latency when thread communicates with the global heap.
\end{enumerate}

\subsection{Local Heap}

Local heap is maintained by each thread. 
It is set up when a thread makes the first memory request.
Metadata are stored in local heap, such as size class and pointers to usable chunks.
Every memory chunk in the local heap can be classified into five categories: in-use, waiting, full, not-available, clean.


\textbf{In-use chunk}. 
When a thread makes an allocation request, 
Wamalloc will first find the appropriate size class, 
then find the currently using memory chunk and allocate memory blocks from it.
All small memory requests are served by in-use chunks.

\textbf{Waiting chunk}. 
Chunks in waiting list of the local heap are called waiting chunks. 
Waiting chunks are the chunks in a partially allocated state. 
When a thread use up the whole memory blocks in current in-use chunk,
waiting chunks of the same class size are candidates waiting to be chosen as a new in-use chunk.

\textbf{Full chunk}. 
After several memory requests of a particular size class, 
a memory chunk may be out of blocks, which is called a full chunk.
No more blocks can be allocated from a chunk in full state, 
thus the following requests will cause the allocator to get another usable chunk.
When a block in a full chunk is freed,
this full chunk turns to a waiting chunk, which will be added to the waiting list of current local heap.

\textbf{Not-available chunk}.
Consider all the things we discussed so far, 
a typical life cycle of a memory chunk could be first used as in-use chunk, then waiting chunk, and back and forth repeatedly. 
For the wear-aware purpose, that is not what we expect. 
Thus we add a wear-count variable to each memory chunk and define an allocation threshold \textbf{WEAR\_LIMIT}.
When a chunk is first allocated from global heap, its wear-count variable is set to zero.
Every time a block is allocated from a chunk, its wear-count variable increases by one. 
If the wear-count variable of a chunk reaches the predefined allocation threshold, 
it is no longer available for allocation in this thread, and its state becomes not-available. 
When all the blocks in a not-available chunk are freed,
the chunk is returned back to global heap.
%for further elaborate wear-leveling policy, 
%which will be discussed in the following subsection.

\textbf{Clean chunk}.
After all the blocks in a waiting chunk are freed, 
the waiting chunk can be used as a clean chunk without a specific size class.
Hence it should be added to the free chunk pool, becoming a candidate for in-use chunk of any size class
How to allocate chunks from the free chunk pool must be carefully designed to achieve a wear-leveling purpose.
%In summary, Figure~\ref{fig:state} shows the states of chunk and their transitions.

%\begin{figure}[h]
%\centering
%\includegraphics[width=3.2in]{state.png}
%\caption{The state machine of chunk state}
%\label{fig:state}
%\end{figure}

In local heap,
each thread maintains an array of in-use memory chunks, 
an array of waiting chunks list, indexed by size class respectively, 
and a free list to hold clean chunks.
The logical view of local heap and its relationship with memory chunks is shown in Figure~\ref{fig:heap}.

\begin{figure}[h]
\centering
\includegraphics[width=3.2in]{heap.png}
\caption{Heap structure}
\label{fig:heap}
\end{figure}

\subsection{Global Heap}

Global heap maintains the whole memory chunks obtained from operating system. 
All the chunks of each thread are allocated from global heap. 
The logic view of global heap is shown in Figure~\ref{fig:global}.

\begin{figure}[h]
\centering
\includegraphics[width=3.2in]{global.png}
\caption{Global heap}
\label{fig:global}
\end{figure}

\subsubsection{Classes of allocatable chunks}
Global heap first requests a huge continuous raw memory from operating system
and divide it into several chunks ready to be allocated to different threads.
All the chunks starting from the free pointer(never allocated) or at the free list(reuse) are ready to be allocated, 
just like what we have discussed in the previous subsection,
how to allocate these free chunks is the key to our wear-leveling policy.
If a chunk is allocated from the free pointer, 
Wamalloc just returns the pointer and then increases the free pointer by the size of memory chunk, 
Otherwise if a chunk is allocated from the free list, a wear-leveling policy must be applied.

\subsubsection{Method of enlarging global heap memory}
Global heap is enlarged via system call(e.g., mmap) if there is no available memory chunks.
Since global heap is shared by all threads, 
a lock is certainly required, indicating a performance descend when lock contention happens.
Thus there exists two different opinions on how to increase the memory space. 
One solution aims at economic usage of process address space, for example, increasing the space linearly;
while the other solution aims at reducing the lock contention if possible, for example, increasing the space exponentially. 
In wamaloc, we use the latter solution for two reasons: 
\begin{enumerate}
    \item With the popularization of 64-bit machines, size of the virtual memory far exceeds the real physical memory an application needs.
Hence, it is unimportant that an application may take too many virtual memory. 
In return we can decrease the potential lock contention by acquiring the lock as less as possible.
    \item Almost all of the modern CPUs support page table mechanism, 
        which means the physical memory is mapped to some virtual address only when an application really needs the physical memory. 
        There is no page mapping at the virtual memory that an application doesn't touch, thus no physical memory is wasted.
\end{enumerate}

\subsubsection{How to reap chunks of terminated threads}
After a thread is terminated, there may be several remaining chunks in thread's local heap. 
These memory chunks need to be handled properly.
A typical method used in general-purpose memory allocator is to push these chunks into a global stack(FILO queue)
for the reason that these chunks are probably just used by the terminated threads and they may be in CPU's L1/L2/L3 cache, 
reusing these chunks is a cache-friendly option.
However, it is not a possible option for non-volatile memory because these operations will reduce the lifetime of NVM.
If we continuously serve a thread with the hottest memory chunk, memory ceil would be damaged quickly.
More details will be discussed in the next subsection.

\subsection{Wear-leveling policy in different level}

Now we will discuss the wear-leveling policy in different level of Wamalloc in detail.
The overall purpose is quite simple: all memory blocks are expected to be allocated evenly.

Traditional method uses first-in-first-out(FIFO) allocation policy(such as queue) 
or LRU\cite{zhou2009durable}\cite{rodriguez2015write}.
Although the advantage of FIFO is simple and fast, it is not an accurate wear-aware policy.
LRU may be better than FIFO in wear-aware accuracy, but the complexity of maintaining a LRU queue is too high which may cause a performance drop.

NVMalloc\cite{moraru2013consistent} proposes a method to tackle this problem.
It timestamps every available free memory blocks to make sure every block is allocated at least in interval $T$(a predefined threshold).
NVMalloc implements it using a don't-allocate list and on every allocation and deallocation,
NVMalloc examines the block at the head of this list.
If it has been in this list for at least time $T$,
it will be removed from the list and marked as available.
Thus, strictly speaking, it is just a modified FIFO and its drawbacks still exist.
Further more, making a memory block not available for the interval of $T$ period will consume more memory spaces,
which is obvious since when there is no available block in the interval of $T$, 
the allocator needs to request more memory from operating system for new coming requests.

In Wamalloc, explicit wear-leveling policies are applied in three different places. 
We propose a novel hybrid wear-leveling method to balance the tradeoff between wear-leveling accuracy and performance.

% wear policy
\subsubsection{In block level}
Our target is to allocate memory blocks as evenly as possible.
Thus it is obviously that Wamalloc should allocate memory block starting from the free pointer first
because these memory blocks have never been allocated since this chunk is assigned to its local heap.
After a chunk runs out of its first-allocated blocks, 
Wamalloc will search the free list of the chunk to check whether any reusable block is available.
When a thread frees a memory block, this block should be added to the free list of its belonging chunk.
Thus, we should apply wear-leveling policy to this free list.
In consideration of allocation latency, a simple and fast method is more appropriate in this situation.
Thus, we consider the free list as a FIFO queue,
which has a $O(1)$ time complexity and a nearly optimal wear-leveling behavior.
FIFO may not give an accurate allocation, but in return the allocation latency is small.

\subsubsection{In local heap level}

When the current in-use chunk runs out of blocks, 
a chunk should be chosen as a new in-use chunk based on the wear-leveling policy.
%According to the state machine shown in the previous section,
There are two sources that an available usable chunk can come from:
one is from the waiting chunk list, the other is from the free list of clean chunk in local heap.

Since Clean chunks don't belong to any size class, 
they are more flexible than the chunks in waiting chunk list.
In other words, if we have some clean chunks in the free list, 
we don't need to interfere with the global heap
because these clean chunks could meet requests of any size class directly.
Thus waiting chunk list will be checked first to find the chunk that could replace the current in-use chunk.

Now we will discuss how to organize the waiting chunk list and the clean chunk list.
The method used here should take two things into consideration:
performance and wear-leveling.
Due to the fine-grained wear-leveling policy that will be described below,
here we put an emphasis on the performance and provide a coarse-grained wear-leveling policy.
Again, We think FIFO is our best choice in this level.

\subsubsection{In global heap level}
It is a common case that global heap needs to reap the memory chunks returned from local heap.
Hence there is also a free chunk list in global heap maintaining all usable chunks. 
Just like the policy adopted in block level, 
when a new thread requests a clean memory chunk from global heap,
Wamalloc will first allocate the memory chunk starting from the free pointer; 
if no available chunk is found, Wamalloc will allocate chunks from free chunk list complied with the following wear-leveling policy.

% hybrid method, 异步四叉堆
The best wear-leveling policy should return the least allocated objects so far. 
To the best of our knowledge, the most suitable data structure is a priority queue using allocation frequency as its key,
and the time complexity of finding the minimum key is $O(1)$, which sounds perfect to our wear-leveling policy.
However, since it is the best, it is strange that no wear-aware NVM allocators use it.
We think the main reason is that its time complexity of the push operation is too high, 
which is $log(n)$, if priority queue is implemented by heaps. 
It will cause an impressive latency when the push operation happens frequently.

In Wamalloc, we use two optimizations make the operations of priority queue not to be the bottleneck:
1) A 4-ary heap is used to implement the priority queue since it has a more CPU-friendlier behavior than other implementations.
2) Chunks are not moved to global pool synchronously, instead, the real action is taken under the ground asynchronously.

We use our optimized priority queue as the wear-leveling policy used in free chunk list of global heap.
% The tradeoff is that it may consume a slight more memory space, which is in an acceptable range 

There are two cases when a thread needs to return memory chunks back to the global heap: 
1) when a thread is terminated;
2) as we have discussed above, the wear-leveling policy in block level is FIFO, which is not an accurate method.
However, we can achieve a nearly optimal policy using following strategy: 
we define a threshold value \textbf{WEAR\_LIMIT} and an allocation time variable \textbf{AllocTimes} per chunk. 
Every time a block is allocated from a chunk, its per chunk variable \textbf{AllocTimes} increases by one.
If \textbf{AllocTimes} exceeds the predefined threshold \textbf{WEAR\_LIMIT}, 
the chunk is no longer available for serving new coming request, 
and state of the chunk becomes to not-available.
When all memory blocks in a not-available chunk are freed, 
this chunk will be moved to the global heap.
Instead of directly inserting the chunk to the priority queue in the global heap, Wamalloc will do it asynchronously.
Specificly, the chunk will be first inserted into a producer-consumer queue, which is a $O(1)$ operations;
a background thread periodically check the producer-consumer queue to get a returned chunk 
and insert it to the priority queue of global heap.
Since the producer-consumer queue is a global object shared by all threads, another lock is needed.
A problem may be raised when lock contention happen.
However, pushing a chunk into the producer-consumer queue is not in the critical path of allocation 
and will not happen very frequently,
which means the possible lock contention penalty will be amortized so that it won't be the bottleneck of Wamalloc.

\subsection{Data Placement In DRAM And NVM}

Metadata in Wamalloc can be classified into two different categories: data description and space description.
In this subsection, we will discuss which category metadata previously mentioned belong to
and how to place them in a hybrid memory system.
In Wamalloc, metadata include chunk header, local heap and global heap.
Chunk header belongs to data description category, 
in which it manages the block informations inside a chunk and some chunk-related data, 
such as the count of remaining free blocks, size class of the current chunk,
chunk owner, chunk state, wear-leveling count of the chunk and so on.
While local heap and global heap belong to space description,
which are used to advise the allocator where can find the appropriate available chunks.

In traditional memory allocators, metadata and data are coupled with each other very tightly.
When an allocation or a deallocation request comes, 
there will be plenty of small metadata writes of changing pointer to chain its previous block and next block 
in order to split large space or merge small continuous spaces into a large free space.
This access pattern will probably decrease the lifetime of NVM because of the frequent writes to metadata.
It is not what we expect in a NVM allocator.

In Wamalloc, we keep all metadata in DRAM except for chunk header. 
Putting Chunk header in NVM is because of the simplicity of finding the appropriate chunk header.
When a thread frees a memory block back to allocator, 
a memory pointer \textbf{ptr} is passed as argument to Wamalloc and 
its chunk header is easily computed by \textbf{ptr}(e.g., a simple subtraction),
otherwise another mechanism should be introduced to 
remember the relationship between a memory address and its chunk header,
which will certainly cause an extra time cost.
At the meantime, Wamalloc keeps the writing frequency of chunk header as low as possible. 
%Figure~\ref{fig:globalview} show the global view of Wamalloc.
%\begin{figure}[ht!]
%\centering
%\includegraphics[width=3.2in]{globalview.png}
%\caption{Global view}
%\label{fig:globalview}
%\end{figure}

\subsection{Allocation Algorithm}

Based on the discussion above, we summarize the allocation algorithm in the following pseudo-code.

\begin{algorithm}
\caption{Allocation Algorithm}\label{euclid}
\begin{algorithmic}[1]
\Procedure{Wamalloc}{size}

\State $\textit{sc} \gets \text{sizeToClass(}\textit{size}\text{);}$

\If {$\text{isSmallClass}(sc)$}
    \State $\text{lh} \gets \text{CurrentLocalHeap();}$
    \State $\text{retry:}$
    \State $\text{ch} \gets \text{InUseChunk(}\textit{lh, sc}\text{);}$
    \State $\text{ptr} \gets \text{AllocFromChunk(}\textit{ch}\text{);}$

    \If {$\text{IsCleanChunk(}\textit{ch}\text{)}$}
        \State $\text{StateOf(}\textit{ch}\text{)} \gets \text{FULL;}$
        \State $\text{FindNewChunkToReplaceInuse(}\textit{lh, sc}\text{);}$
        \If {$\text{IsNULL(}\textit{ptr}\text{)}$}
            \State \textbf{goto} \emph{retry};
        \EndIf
    \EndIf

    \State \text{IncreaseWearCount(}\textit{ch, 1}\text{);}
    \If {$\text{WearCount(}\textit{ch}\text{)} \ge \textit{WEAR\_LIMIT}}$
        \State $\text{StateOf(}\textit{ch}\text{)} \gets \text{NotAvailable;}$
        \State $\text{FindNewChunkToReplaceInuse(}\textit{lh, sc}\text{);}$
    \EndIf

\Else
    \State $\text{ptr} \gets \text{LargeAlloc(}\textit{size}\text{);}$
\EndIf
\Return $\text{ptr;}$

\EndProcedure
\end{algorithmic}
\end{algorithm}

Wamalloc will first compute the size class according to the parameter \textbf{size}, 
then use this value to find the appropriate in-use chunk.
If it is a small size request, allocation will happen directly in the current in-use chunk.
Three exceptions could occur during an allocation:
1) the first time a thread makes a request on class $n$, no in-use chunk is available in the current local heap;
2) the in-use chunk is out of free blocks;
3) when the wear-leveling variable reaches the predefined threshold.
All these exceptions cause the same action:
the current in-use chunk(in the first situation, it is a NULL pointer) should be replaced by another usable chunk.
If it is a large size request, the request will be redirect to operating system.

The critical path in allocation algorithm is short, which guarantees the performance stability.

\subsection{Deallocation Algorithm}

we summarize the deallocation algorithm in Algorithm 2.

\begin{algorithm}
\caption{Deallocation Algorithm}\label{euclid}
\begin{algorithmic}[1]
\Procedure{wafree}{ptr}

\State $\text{ch} \gets \text{ExtractHeader(\textit{ptr});}$
\State $\text{lh} \gets \text{CurrentLocalHeap();}$

\If {$\text{IsLarge(\textit{ch})} $}
\State \text{FreeLarge(\textit{ch})}
\Else
\State \text{FreeSmall(\textit{ch})}
\EndIf

\EndProcedure
\end{algorithmic}
\end{algorithm}

If it is a large memory deallocation, the memory will be returned to operating system directly via system call.
Otherwise it is a small memory deallocation, Wamalloc will perform different actions based on the current in-use chunk state.
Specificly, there are four cases: if the current chunk is 
(1) in in-use state, nothing will be done;
(2) in full state, it will be added to the waiting list of its local heap;
(3) in waiting state and the chunk becomes to a clean chunk, it will be added to the free list of its local heap;
(4) in not-available state and the chunk becomes to a clean chunk, it will be returned to the free list of global heap.

\section{Evaluation}

In this section, we evaluate Wamalloc in several benchmarks and compare it against the glibc allocator and NVMalloc.
We analyze wear-leveling as well as physical memory consumption and the performance of Wamalloc 
under uniform and random allocation workloads.

\subsection{Benchmark Setup}

All evaluations were performed on a linux machine with a Intel Core i7 processor and 8GB DRAM, running the Linux 3.16 kernel.
It is important to note that the evaluation software for NVM is not necessary for our evaluations, 
for the reason that Wamalloc focuses on the wear-leveling of allocation and the allocator performance, rather than the performance of different NVM.
Thus We use DRAM as NVM proxy.
If the conclusions are true performed on DRAM, it is also true performed on NVM.
Wamalloc is implemented in about 800 lines of code in C.
The \textbf{WEAR\_LIMIT} global variable is set to $50000$ in all evaluations.

We decided to evaluate wear-leveling against NVMalloc and allocation performance against NVMalloc, as well as glibc,
which is compared to provide a performance baseline. 
The only three non-volatile allocator implementations publicly available at the time of writing are 
NVMalloc, nvm\_malloc\cite{schwalbnvm} and pmemalloc.
Nvm\_malloc provided a recovery mechanism and used the Persistent Memory Emulation Platform(PMEP)\cite{dulloor2014system} to emulate the performance of emerging NVM technologies.
Since Wamalloc uses DRAM as a proxy and recovery is not supported, we exclude nvm\_malloc for fairness reasons.
Pmemalloc uses a simple first-fit algorithm and thus performs very poorly at runtime costs with the increase of allocations.
NVMalloc focuses largely on designing a wear-aware allocation algorithm to increase the lifetime of NVM.
Our assumption, evaluation setup and the targeted problem are closest to these in NVMalloc, 
thus we will treat NVMalloc as our main contender.

\subsection{wear-leveling}

First, we evaluate the wear-leveling ability of our allocator
using a simple test program under uniform and random allocation and deallocation operations(50\% each).
In NVMalloc, for every allocation, the program writes the allocated block once, then use Pin\cite{luk2005pin} to record store to memory and compared the total writes of each block.
In this method, the total writes depend on not only the allocator itself but also the behavior of test program.
%which we think may not be necessary and even worse, not be accurate in evaluating the wear-leveling ability of memory allocator.
Instead, it is more accurate to evaluate the allocation frequency of each memory block 
returned by the allocator to illustrate the ability of wear-leveling.

\begin{figure}[t]
\centering
\includegraphics[width=3.2in]{eval_1.png}
\caption{Uniform allocations(128byte)}
\label{fig:eval_1}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=3.2in]{eval_3.png}
\caption{Random allocations between 64 and 512 bytes}
\label{fig:eval_3}
\end{figure}

More specificly, we run tests to get all of allocation memory addresses, 
then store these addresses into a set \textbf{V},
a collection of distinct memory addresses.
For example, if one specific memory address is allocated twice by allocator, the address appears only once in set \textbf{V}.
Then we use (total allocation times)/(sizeof \textbf{V}) as average allocation frequency per block to evaluate the wear-leveling ability.

Figure~\ref{fig:eval_1} and Figure~\ref{fig:eval_3} show the result of wear-leveling 
in terms of average allocation frequency per block
under uniform allocation of 128 bytes and random allocation between 64 and 512 bytes.
Since Wamalloc use a more elaborate and accurate wear-leveling policy than NVMalloc,
it is expected that Wamalloc performs better than NVMalloc.

As the evaluations show, in Figure~\ref{fig:eval_1}, as the times of allocation increases,
Wamalloc performs better than NVMalloc under the workloads of uniform allocation.
In Figure~\ref{fig:eval_3},
the average allocation frequency per block of NVMalloc is about 2.
While the value of that in Wamalloc ranges from 1.4 to 1.9, 
indicating that Wamalloc performs better than NVMalloc under the workload of random allocation,
which can be explained by using the 4-ary heap implementing an accurate and elaborate wear-leveling policy.

However, it is meaningless to compare the average allocation frequency without considering the physical memory consumption.
A simple allocator that allocates every memory block just once and never use that memory again 
could make the average allocation frequency per block to be 1, 
while at the meantime it consumes plenty of physical memory, causing it to be a useless allocator.
Thus, next we are also supposed to measure the memory consumption of above evaluations under uniform and random allocation.

Figure~\ref{fig:eval_2} and Figure~\ref{fig:eval_4} show the total memory consumption under the same workloads 
as that in Figure~\ref{fig:eval_1} and Figure~\ref{fig:eval_3}.
We can conclude from these results that 
Wamalloc is more economic in physical memory usage than NVMalloc.
Under uniform allocations, NVMalloc consumes nearly twice more memory than Wamalloc.
Under random allocations, the growth rate of memory consumption of NVMalloc and Wamalloc tend to be linear 
as the allocation time increases. 
However, Wamalloc still consumes less physical memory than NVMalloc by a constant difference.

\begin{figure}[t]
\centering
\includegraphics[width=3.2in]{eval_2.png}
\caption{Physical memory consumption under uniform allocations(128byte)}
\label{fig:eval_2}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=3.2in]{eval_4.png}
\caption{Physical memory consumption under random allocations}
\label{fig:eval_4}
\end{figure}

In the above evaluations, we can arrive the conclusion that
Wamalloc performs better than NVMalloc not only in the wear-leveling policy,
but also in physical memory consumption.

\subsection{allocation performance}

We compare the allocation performance of Wamalloc against different allocators,
which is a crucial metric since it will affect the overall performance of a program.
Specificly, we compare the average allocation latency under workloads of different parallel threads against NVMalloc.
We also add the gblic malloc(version 2.21) into comparison to provide a performance baseline.
Unfortunately, NVMalloc does not support multithreading,
so we modify the source code of NVMalloc to make multithreading supported.

First, we evaluate the average allocation latency as the number of parallel threads increases
under the workload of uniform(128 bytes) and random(64 and 512 bytes) allocation without freeing any memory block.
Since Wamalloc uses a thread-cache structure and has a very short critical path using locks as few as possible,
the average allocation latency should be little related to the number of threads.
These expectations are confirmed by the result of our evaluations shown in Figure~\ref{fig:eval_5} and Figure~\ref{fig:eval_6}.
From the result we can verify that Wamalloc outperforms both glibc and NVMalloc under different workloads.


\begin{figure}[t]
\centering
\includegraphics[width=3.2in]{eval_5.png}
\caption{Average allocation latency under uniform allocations(128byte)}
\label{fig:eval_5}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=3.2in]{eval_6.png}
\caption{Average allocation latency under random allocations}
\label{fig:eval_6}
\end{figure}

Next, a second benchmark was run in which the program is extended to free all the memory after allocations, 
the runtime of which was included in the measurement of average allocation latency.
Allocating and then freeing is a common scenario in most applications, 
and it is more complicated than just allocating the memory in a multithread environment 
for the reason that freeing may cause changes of the state of some chunks or even some movements of chunk from local heap to global heap.
The evaluation result is shown in Figure~\ref{fig:eval_7}.
Malloc in Glibc performs better than itself when freeing is involved, 
but the performance still falls behind Wamalloc by a small constant gap.

\begin{figure}[t]
\centering
\includegraphics[width=3.2in]{eval_7.png}
\caption{Performance comparison when freeing memory after allocations under random allocations}
\label{fig:eval_7}
\end{figure}

\subsection{Evalution Summary}

As demonstrated, 
the wear-leveling ability of Wamalloc performs better than NVMalloc.
In terms of physical memory consumption, Wamalloc also outperforms NVMalloc
due to the smart memory reuse policy.

We also show that Wamalloc's allocation performance in a multithreading environment is not only better than NVMalloc,
but can also outperform glibc malloc, which is the standard system allocator in most systems.
Under the random pressure of allocating and freeing memory simulating the actual program behavior,
Wamalloc also performs better than NVMalloc and glibc malloc.

\section{Related work}

The emerging technology of NVM has gained more and more attentions in research community.
Propositions were made for different levels of computer system, 
such as using NVM as a fast buffer for flash-based storage\cite{greenan2007prims}, 
or as a supplement for DRAM exploiting the benefit of the hybrid memory system.

In the research area of how to extend the endurance of NVM,
plenty of works have been done at different levels.

At hardware level, ping zhou et al.\cite{zhou2009durable} proposed a suite of hierarchical techniques to
prolong the lifetime of PCM-based main memory: 
redundant bit-write removal, row shifting, and segment swapping.
Lei Jiang et al.\cite{jiang2012improving} proposed write truncation (WT) and form switch (FS),
which is a way to avoid fully writing a small set of difficult-to-write cells, to speedup write and read operations in MLC PCM.
Qureshi, Moinuddin K et al.\cite{qureshi2009enhancing} and Shao, Zili et al.\cite{shao2012ptl} proposed adding a translation layer
between the physical address and the logical address in the memory controller.
Ferreira, Alexandre P et al.\cite{ferreira2010increasing} proposed a technique that involved 
writeback minimization with new cache replacement policies, and wrote only the bit actually changed.

At software level, there are also much works proposed to minimize to the memory writes to NVM, 
such as the research work by Hu, Jingtong et al.\cite{hu2013software}.
Early in the time of research in NVM, 
mnemosyne\cite{volos2011mnemosyne} and NV-heaps\cite{coburn2011nv} provided complete suites to work with NVM, 
which involved kernel modifications, user mode libraries and so on.
However, the endurance problem is not considered too much.

A more specific NVM allocator is NVMalloc by Moraru et al.\cite{moraru2013consistent},
intended to be an efficient, general-purpose NVM allocator with a wear-leveling policy using timestamp.
More specificly, NVMalloc curbed the write frequency of memory block no higher than 1/T, which is a predefined threshold.
Further more, NVMalloc decoupled metadata and data to reduce metadata write.
The focus of NVMalloc is mainly on wear-aware algorithms to increase the lifetime of NVM.

Another NVM allocator is nvm\_malloc proposed by Schwalb et al.\cite{schwalbnvm},
presenting a general-purpose NVM allocation concept with fast built-in recovery features
as well as fine-grained access to NVM in a safe fashion.
Introducing recovery features causes nvm\_malloc incurring higher costs
because of the reserve/activate mechanism and the lack of the maturity as a research prototype mentioned by its authors.
A follow-up comparison to the mature version of nvm\_malloc is subject to future work.

\section{Conclusion and future work}

In this paper, we observe the fact that
no NVM allocators at the time of writing 
can provide both an accurate wear-leveling policy and a good allocation performance simultaneously.
Based on the observation, we propose an efficient wear-aware NVM allocator, Wamalloc,
which uses a thread-cache memory architecture causing nearly no lock contention in critical path,
and an elaborate hybrid wear-leveling policy to improve lifetime of NVM.
The experimental results show that
Wamalloc outperforms NVMalloc in terms of wear-leveling policy, total memory consumption and allocation performance.
%Wamalloc also outperforms Glibc Malloc in terms of allocation performance.
In our future work, we will continue to optimize the wear-leveling policy based on our hybrid method.
Performance scalability is also subject to future work.

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



% use section* for acknowledgement
%\section*{Acknowledgment}

%The authors would like to thank...


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\bibliographystyle{IEEEtran}
\bibliography{IEEE_paper}


% that's all folks
\end{document}


