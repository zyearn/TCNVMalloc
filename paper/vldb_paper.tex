% THIS IS AN EXAMPLE DOCUMENT FOR VLDB 2012
% based on ACM SIGPROC-SP.TEX VERSION 2.7
% Modified by  Gerald Weber <gerald@cs.auckland.ac.nz>
% Removed the requirement to include *bbl file in here. (AhmetSacan, Sep2012)
% Fixed the equation on page 3 to prevent line overflow. (AhmetSacan, Sep2012)

\documentclass{vldb}
\usepackage{graphicx}
\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)


\begin{document}

% ****************** TITLE ****************************************

\title{A Sample {\ttlit Proceedings of the VLDB Endowment} Paper in LaTeX
Format\titlenote{for use with vldb.cls}}

% possible, but not really needed or used for PVLDB:
%\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as\textit{Author's Guide to Preparing ACM SIG Proceedings Using \LaTeX$2_\epsilon$\ and BibTeX} at \texttt{www.acm.org/eaddress.htm}}}

% ****************** AUTHORS **************************************

% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{8} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.

\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Ben Trovato\titlenote{Dr.~Trovato insisted his name be first.}\\
       \affaddr{Institute for Clarity in Documentation}\\
       \affaddr{1932 Wallamaloo Lane}\\
       \affaddr{Wallamaloo, New Zealand}\\
       \email{trovato@corporation.com}
% 2nd. author
\alignauthor
G.K.M. Tobin\titlenote{The secretary disavows
any knowledge of this author's actions.}\\
       \affaddr{Institute for Clarity in Documentation}\\
       \affaddr{P.O. Box 1212}\\
       \affaddr{Dublin, Ohio 43017-6221}\\
       \email{webmaster@marysville-ohio.com}
% 3rd. author
\alignauthor Lars Th{\Large{\sf{\o}}}rv{$\ddot{\mbox{a}}$}ld\titlenote{This author is the
one who did all the really hard work.}\\
       \affaddr{The Th{\large{\sf{\o}}}rv{$\ddot{\mbox{a}}$}ld Group}\\
       \affaddr{1 Th{\large{\sf{\o}}}rv{$\ddot{\mbox{a}}$}ld Circle}\\
       \affaddr{Hekla, Iceland}\\
       \email{larst@affiliation.org}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor Lawrence P. Leipuner\\
       \affaddr{Brookhaven Laboratories}\\
       \affaddr{Brookhaven National Lab}\\
       \affaddr{P.O. Box 5000}\\
       \email{lleipuner@researchlabs.org}
% 5th. author
\alignauthor Sean Fogarty\\
       \affaddr{NASA Ames Research Center}\\
       \affaddr{Moffett Field}\\
       \affaddr{California 94035}\\
       \email{fogartys@amesres.org}
% 6th. author
\alignauthor Charles Palmer\\
       \affaddr{Palmer Research Laboratories}\\
       \affaddr{8600 Datapoint Drive}\\
       \affaddr{San Antonio, Texas 78229}\\
       \email{cpalmer@prl.com}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
\additionalauthors{Additional authors: John Smith (The Th{\o}rv\"{a}ld Group, {\texttt{jsmith@affiliation.org}}), Julius P.~Kumquat
(The \raggedright{Kumquat} Consortium, {\small \texttt{jpkumquat@consortium.net}}), and Ahmet Sacan (Drexel University, {\small \texttt{ahmetdevel@gmail.com}})}
\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.


\maketitle

\begin{abstract}
The abstract for your paper for the PVLDB Journal submission.
The template and the example document are based on the ACM SIG Proceedings  templates. This file is part of a package for preparing the submissions for review. These files are in the camera-ready format, but they do not contain the full copyright note.
Note that after the notification of acceptance, there will be an updated style file for the camera-ready submission containing the copyright note.
\end{abstract}



\section{Introduction}

Dynamic random-access memory(DRAM) has been used as the main memory of computer system for a long time, which is a type of random-access memory that stores each bit of data in a separate capacitor within an integrated circuit. 
However, with the great demand of performance and enerygy-constrain application, the defects of DRAM has become a main concern, for example, the limited density and its high energy consumption. 
In order to address these problems, an emerging new technology named non-volatile memory(NVM) has been proposed, such as Phase-change memory(PCM), spin torque, and memristor. 
NVM has the advantages of low latency, low power consumption and byte-addressable, which makes it a better alternative when compared  to DRAM.

However, some non-volatile memories suffers from limited write endurance. 
Let we take PCM as example. A typical PCM cell permanently fails after nearly $10^7$ to $10^9$ writes. 
In such circumstance, some write-intensive applications will break the PCM in a short time. 
Moreover, many hardware-level wear-awareness policies have been proposed. 
But the popularization of these specified hardware is much slower than the popularization of NVM in the near future. 
What’s more worse, the legacy hardware system then can not use NVM. 
In order to tackle ``write-too-intensive'' problem and support a software-level wear-aware memory allocator, a new memory allocator targeted for NVM platform must be designed and used in nowadays’ applications running on NVM.

Moreover, present wear-aware allocators without hardware-level wear-leveling also don’t support global wear-leveling. 
For example, if an application calls malloc-like interface to get a writeable PCM address, excessive writes to this PCM cell can rapidly destroy it. 
This is not the thing that a memory allocator can control without explicit hardware modifications. It remains a problem we must solve otherwise PCM will be damaged much earlier than we expect.

In this paper, we propose a wear-aware allocator that provide wear-awareness without degrading the performance of allocator compared with state-of-the art allocator. 
Specifically, tcwamalloc concerns three main folds of metadata memory write-reduction, thread arenas with a dedicated wear-aware allocation policy and APIs to support global. 
First, it decouples metadata and data management. Generally speaking, putting metadata into DRAM will decrease plenty of writes to NVM. 
Besides, we provide a wear-aware memory allocation policy without degrading the performance when compared to other NVM and DRAM allocators. 
At last, tcwamalloc implement a mechanisms that allow users do the global wear-leveling without the help of modifications in hardware.

Our contributions can be summarized as follows:
\begin{enumerate}
    \item metadata %分离
    \item % 粗细粒度结合的wear, wait-free(后台线程来把list中的chunk放入优先级队列)
    \item global API wear
    \item experiment...
\end{enumerate}

We have implemented a prototype version of tcwamalloc and evaluated its performance and some critical aspects against state-of-the-art memory allocators. 
Experiment shows that for most cases tcwamalloc outperforms prior system in average block allocation times and allocation latency.

\section{Background and Motivations}

不用用glibc，因为重用
用USING, WAITING 代替 fore，back

\section{Design of tcwamalloc}

In this section, we propose tcwamalloc, an memory allocator for non-volatile memory with the dedicated design for wear-aware purpose without the descend of allocation performace.

\subsection{Design Principles}

\begin{enumerate}
    \item The most important design goal of tcnvmalloc is wear-levelling. Hence, every time we call tcwamalloc’s API to gain a writeable address, a memory block with least allocation times is expected to return to us. However, a strict policy which return the exact block with least allocation times may take plenty of latencies. ln the later section will see a great tradeoff to tackle this problem.
    \item Another requirement is low latency, otherwise the memory would probably become the bottleneck of the entire program, which results in the allocator to be useless. To reduce the allocation latency, a local heap should be used by every thread to reduce the possible lock contention, and all memory managed by thread local heap should be allocated from a global heap. however, using lock when request/free memory from global heap is inevitable, as we will see, this is the only lock we use in the whole allocator.
\end{enumerate}

\subsection{Overview Stucture}

In this subsection, we will dig deeper into the design architecture of tcwamalloc.
The stucture of tcwamalloc is showed in Figure xx.
tcwamalloc maintains a local heap for each thread, and a global heap for all threads.
The reason why we use such a design is from the obvious observation that if a request from a thread can be satisfied by its local heap,
the allocation latency will be very low since there is no lock contention which may degrade the performance.
However, there must be a global heap that manage all the memory that maintained by each thread,
therefore a lock must be involved. One of our implementation focus is emphasized on how to reduce the use of lock to the best of our ability.

[img]overall architecture

Each local heap contains several fix-sized memory chunk.
Each memory chunk is further divided into several memory block according to the size class.
Each thread maintains an array of in-use memory chunk, an array of list that contains a list of waiting chunk and a free list to hold empty chunk.
When a thread makes a memory request of some certain memory class and there is no corresponding in-use chunk to answer that request,
the corresponding chunk in waiting list is examined to become a in-use chunk. 
If still no usable memory chunks can be found,
the free list in the local heap or global heap will be visited based on our wear-aware policy,
which will be discussed in the further subsection.

\subsection{Memory Chunk}

% intro
Memory chunk is the basic building block that would be transferred between global heap and local heap. 
A memory chunk consists of two parts: a small chunk header and a 64KB chunk body. 
Chunk body is divided into several memory block based on the size class,
obviously, with smaller size class comes more memory blocks. 
Chunk header stores some metadata such as remaining free blocks number, size class, chunk owner and so on.

% classes of allocatable objects
Since a memory chunk body only contains 64KB memory, all the memory request smaller than 64KB is regarded as small request,
which is directly handled by local heap without additional locking in the global heap and 
the usable memory block in a memory chunk can be classified into two categories: never-allocated objects and reusable objects,
shown in the figure xx. 
All the blocks starting from the free pointer where never-allocated objects starts or at the free list storing reusable objects is ready to be allocated to users.

[img] chunk header指向chunk内存块的关系

% wear policy
How to allocate these free blocks is the key to our wear-aware strategy, which will be discussed in following subsections...粗细粒度？

% how to choose chunk size
The memory chunk size is very critical to the performance of wamalloc. 
If the chunk size is too small, the memory block will be used up very soon and frequently communicating with the global heap will greatly decrease the performance.
If the chunk size is too large, lots of memory blocks may remain clean or unused, which leads to a huge memory waste. 
A working method used by many other allocator is to choose chunks of small size to serve small blocks, and chunks of large size to serve large blocks.
The reason why they adopt this method is that there will always be avaliable usable memory blocks no matter what size class it belongs to.
The drawbacks are also obvious.
Since the memory chunk size is different, it increases the complexity of global heap to manage these chunks.
Consequently this will cause the extra time plenalty and the possible memory fragment, which may lead to more physical memory than an application realy needs.

Take the above discussions into consideration, we use an uniform memory chunk size for all size class. 
In most cases, memory requests are small allocations, which are naturly guaranteed that there are several usable memory blocks. 
For a special case, if a memory request is 64KB, there is only one block in the entire chunk body, so the allocation policy just view this block as the last remaining block and return it to the application. After that if the application needs a new 64KB memory, a new chunk is allocated to serve this request.
Further, This design gives us several other benifits: 
1) Since all memory chunks are of the same size, then an empty chunk can easily be transferred from one size class to another without any particular actions.
The only overhead is to modify the metadata of the chunk, which can be negligible when compared to the non-uniform chunk size schema. 
2) The uniform memory chunk simplified the design of global heap, which results in a lower time latency when communicating with global heap.

\subsection{Local Heap}

Local heap is maintained by each thread. 
It is set up when a thread issues the first memory request.
Size class related metadata is stored in local heap, every chunk in the local heap can be classified into four categories: Inuse, Waiting, Full, Notavailable. 
The logical view of local heap and its relationship with memory chunk is shown in Figure xxx.

[img] local heap..

\textbf{Inuse chunk}. When an application request a memory block, wamalloc will first find the appropriate size class, then use this class number as index to get the currently using memory chunk. 
All small memory request is served by the inuse chunk.

\textbf{Waiting chunk}. Chunks in the wait list are called waiting chunks. Waiting chunks are the chunks in partially allocated state. 
When application use up the whole memory blocks in the current chunk, chunk of the same class size is a candidate waiting to be chosen as a new inuse chunk.

\textbf{Full chunk}. After several memory requests to a particular size class, a memory chunk may be out of blocks, which is called a full chunk.
No more blocks can be allocated from a chunk in full state , thus the following request will cause the allocator to get usable chunk from waiting chunks.
When a block is freed and the chunk to which the block belongs is in full state, this full chunk is added to the waiting list, becoming a waiting chunk.

\textbf{Notavailable chunk}. Consider all the things we discussed so far, a typical life cycle of a memory chunk could be first used as inuse chunk, then waiting chunk, and back and forth repeatedly. 
For the wear-aware purpose, that is not we expect. 
So we add a wear-count value to each memory chunk and define an allocation threshhold. 
When a chunk is first allocated from global heap, its wear-count value is set to zero.
Every time a block is allocated from a chunk, its wear-count value increases by one. 
If wear-count value of a chunk reaches the predefined allocation threshhold, it is no longer available for allocation, and its state becomes Notavailable. 
When all the blocks in a Notavailable chunk are freed, this chunk is returned to global heap for furthur elaborate wear-leveling policy, which will be discussed in the following subsection.

The state machine of the relationship between different kinds of chunk is shown in figure xxx.

[img] state machine

\subsection{Global Heap}

Global heap maintains the whole memory chunks obtained from operating system. 
All the chunks of each thread are allocated from global heap. 
The logic view of global heap is shown in figure xxx.

[img] global heap

From the figure we can conclude that wamalloc first requests a huge continous raw memory and divide it into several chunks ready to be allocated to different threads.
All the chunks starting from the free pointer or at the free list is ready to be allocated.
Just like what we have discussed in block allocation, how to allocate these free chunks is the key to our wear-aware strategy, which will be discussed in following subsections. If a chunk is allocated from the free pointer, wamalloc just returns the pointer and then increases this pointer by the size of memory chunk. Otherwise a chunk is allocated from the free list, a wear-leveling policy must be applied.

Global heap is enlarged via system call(e.g., mmap) if there is no available memory chunk.
Since global heap is shared by all threads, a lock is certainly required, which means a performance descend when lock contention happens.

Thus there exists two different opinions on how to increase the memory space. 
One possible solution aims at economic usage of virtual memory, specifically, increasing the space linearly;
while the other solution aims at reducing the lock contention if possible, for example, increasing the space exponentially. 
In wamaloc, we use the latter solution for two reasons: 
1) With the popularization of 64-bit machine, the size of virtual memory far exceeds the real physical memory an application needs.
Hence, it is unimportant that an application may take too much virtual memory. In return we can probably decrease the potential lock contention by accquiring the lock as less as possible.
2) Almost all of the modern CPUs supports page table mechanism, which means the physical memory is mapped to some virtual address only when an application really needs the physical memory. There is no page mapping at the virtual memory that an application doesn't touch, indicating that no physical memory is wasted.

After a thread is terminated, there may be several chunks in thread's local heap. 
We need to handle these remaining memory chunks properly.
A typical method used in general-purpose memory allocator is to push these chunks into a global stack(FILO queue)
for the reason that these chunks are probably just used by the terminated thread and they may be in cpu's L1/L2/L3 cache, which is a cache-friendly option to return the chunk already in cache.
However, it is not a possible option for non-volatile memory because wear-leveling must be taken into consideration.
If we continuously serve an application with the hottest memory chunk, memory ceil would be damaged quickly.
Thus we should use

\subsection{Data Placement In DRAM And NVM}

%metadata write is 频繁
%why chunk header is in NVM?

\subsection{wear-aware policy in different level}

hybird method

%每个模块的wear-aware策略都不一样, tradeoff,比较内存总消耗。
%block级别：在一般的分配器中，优先分配chunk中free_list的部分
%chunk级别：FIFO？当allocation次数超过一个阈值，还给
%global chunk
%用最简单的，一个block只能分配一次，没意义，内存耗太大了。



\subsection{Allocation Algorithm}

\subsection{Deallocation Algorithm}

\section{Global wear levelling}

why? we can’t know the access pattern of a program.

\section{Evaluation}

\section{Related work}


\section{Conclusions}
This paragraph will end the body of this sample document.
Remember that you might still have Acknowledgments or
Appendices; brief samples of these
follow.  There is still the Bibliography to deal with; and
we will make a disclaimer about that here: with the exception
of the reference to the \LaTeX\ book, the citations in
this paper are to articles which have nothing to
do with the present subject and are used as
examples only.
%\end{document}  % This is where a 'short' article might terminate

% ensure same length columns on last page (might need two sub-sequent latex runs)
\balance

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
This section is optional; it is a location for you
to acknowledge grants, funding, editing assistance and
what have you.  In the present case, for example, the
authors would like to thank Gerald Murray of ACM for
his help in codifying this \textit{Author's Guide}
and the \textbf{.cls} and \textbf{.tex} files that it describes.


% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{vldb_paper}  % vldb_sample.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references

%APPENDIX is optional.
% ****************** APPENDIX **************************************
% Example of an appendix; typically would start on a new page
%pagebreak

\begin{appendix}
You can use an appendix for optional proofs or details of your evaluation which are not absolutely necessary to the core understanding of your paper. 

\section{Final Thoughts on Good Layout}
Please use readable font sizes in the figures and graphs. Avoid tempering with the correct border values, and the spacing (and format) of both text and captions of the PVLDB format (e.g. captions are bold).

At the end, please check for an overall pleasant layout, e.g. by ensuring a readable and logical positioning of any floating figures and tables. Please also check for any line overflows, which are only allowed in extraordinary circumstances (such as wide formulas or URLs where a line wrap would be counterintuitive).

Use the \texttt{balance} package together with a \texttt{\char'134 balance} command at the end of your document to ensure that the last page has balanced (i.e. same length) columns.

\end{appendix}



\end{document}
