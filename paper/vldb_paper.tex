% THIS IS AN EXAMPLE DOCUMENT FOR VLDB 2012
% based on ACM SIGPROC-SP.TEX VERSION 2.7
% Modified by  Gerald Weber <gerald@cs.auckland.ac.nz>
% Removed the requirement to include *bbl file in here. (AhmetSacan, Sep2012)
% Fixed the equation on page 3 to prevent line overflow. (AhmetSacan, Sep2012)

\documentclass{vldb}
\usepackage{graphicx}
\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\begin{document}

% ****************** TITLE ****************************************

%\title{A Sample {\ttlit Proceedings of the VLDB Endowment} Paper in LaTeX
%Format\titlenote{for use with vldb.cls}}
\title{WaMalloc: An Locality-conscious wear-aware allocator for Non-volatile Main Memory}

% possible, but not really needed or used for PVLDB:
%\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as\textit{Author's Guide to Preparing ACM SIG Proceedings Using \LaTeX$2_\epsilon$\ and BibTeX} at \texttt{www.acm.org/eaddress.htm}}}

% ****************** AUTHORS **************************************

% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.

\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Ben Trovato\titlenote{Dr.~Trovato insisted his name be first.}\\
       \affaddr{Institute for Clarity in Documentation}\\
       \affaddr{1932 Wallamaloo Lane}\\
       \affaddr{Wallamaloo, New Zealand}\\
       \email{trovato@corporation.com}
% 2nd. author
\alignauthor
G.K.M. Tobin\titlenote{The secretary disavows
any knowledge of this author's actions.}\\
       \affaddr{Institute for Clarity in Documentation}\\
       \affaddr{P.O. Box 1212}\\
       \affaddr{Dublin, Ohio 43017-6221}\\
       \email{webmaster@marysville-ohio.com}
% 3rd. author
\alignauthor Lars Th{\Large{\sf{\o}}}rv{$\ddot{\mbox{a}}$}ld\titlenote{This author is the
one who did all the really hard work.}\\
       \affaddr{The Th{\large{\sf{\o}}}rv{$\ddot{\mbox{a}}$}ld Group}\\
       \affaddr{1 Th{\large{\sf{\o}}}rv{$\ddot{\mbox{a}}$}ld Circle}\\
       \affaddr{Hekla, Iceland}\\
       \email{larst@affiliation.org}
%\and  % use '\and' if you need 'another row' of author names
% 4th. author
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
\additionalauthors{Additional authors: John Smith (The Th{\o}rv\"{a}ld Group, {\texttt{jsmith@affiliation.org}}), Julius P.~Kumquat
(The \raggedright{Kumquat} Consortium, {\small \texttt{jpkumquat@consortium.net}}), and Ahmet Sacan (Drexel University, {\small \texttt{ahmetdevel@gmail.com}})}
\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.


\maketitle

\begin{abstract}
The abstract for your paper for the PVLDB Journal submission.
The template and the example document are based on the ACM SIG Proceedings  templates. This file is part of a package for preparing the submissions for review. These files are in the camera-ready format, but they do not contain the full copyright note.
Note that after the notification of acceptance, there will be an updated style file for the camera-ready submission containing the copyright note.
\end{abstract}



\section{Introduction}

Dynamic random-access memory(DRAM) has been used as the main memory of computer system for a long time, which is a type of random-access memory that stores each bit of data in a separate capacitor within an integrated circuit. 
However, with the great demand of performance and enerygy-constrain application, the defects of DRAM has become a main concern, for example, the limited density and its high energy consumption. 
In order to address these problems, an emerging new technology named non-volatile memory(NVM) has been proposed, such as Phase-change memory(PCM), spin torque, and memristor. 
NVM has the advantages of low latency, low power consumption and byte-addressable, which makes it a better alternative when compared  to DRAM.

However, some non-volatile memories suffers from limited write endurance. 
Let us take PCM as example. A typical PCM cell permanently fails after nearly $10^7$ to $10^9$ writes. 
In such circumstance, a poorly designed allocator will break the PCM in a short time. 
%Moreover, many hardware-level wear-leveling policies have been proposed. 
%But the popularization of these specified hardware is much slower than the popularization of NVM in the near future. 
%What’s more worse, the legacy hardware system then can not use NVM. 
In order to tackle this problem and support a software level wear-aware memory allocator, a new memory allocator targeted for NVM platform must be designed and used in nowadays’ applications running on NVM.

In this paper, we propose a wear-aware allocator that provide wear-awareness without degrading the performance of allocator compared with state-of-the art allocator. 
Specifically, wamalloc concerns three main aspects of allocator:
(1) a hybrid(fine-grained and coarse-grained) and efficient wear-aware allocation policy;
(2) application level wear-leveling;
(3) metadata memory write-reduction.
First, we provide a wear-aware memory allocation policy without degrading the performance when compared to other NVM and DRAM allocators.
Besides, we solve the application level wear-leveling problem by adding a level of indirection.
If an application calls malloc-like function to get a writeable PCM address, excessive writes to this specific PCM memory could rapidly destroy it. 
This is not what a memory allocator can control without explicit hardware modifications.
It remains a problem we must solve otherwise PCM will be damaged much earlier than we expect.
Thus, we design a method that allow applications do the wear-leveling without the modifications in hardware.
At last, wamalloc decouples metadata and data management.
Generally speaking, putting metadata into DRAM will decrease plenty of writes to NVM, which will probably extend the lifetime of PCM.

Our contributions can be summarized as follows:
\begin{enumerate}
    \item We propose the wamalloc, which
        (1) uses a hybrid wear-leveling policy focusing on an accurate wear-leveling strategy to prevent some specific memory from becoming too hot;
        (2) has a good performance compared to the state-of-the-art allocator;
        (3) decouples metadata and data management to reduce huge small writes.
    \item Considering application level wear-leveling, 
        we propose a mechanism to handle the problem that too many writes to a specific memory address will cause NVM to be damaged more quickly.
        As we can see in the later section, this mechanism can be easily implemented using the wamalloc allocator.
    \item We implement wamalloc as a userspace library, which depends on no special requirements on operating system and hardware modifications.
\end{enumerate}

We have implemented a prototype version of wamalloc and evaluated its performance and some critical aspects against state-of-the-art memory allocators. 
Experiment shows that for most cases wamalloc outperforms prior system in average block allocation times and allocation latency.

\section{Background and Assumptions}

TODO.
[img] NVRAM在系统中的位置

\section{Design of wamalloc}

In this section, we propose wamalloc, an memory allocator for non-volatile memory with the dedicated design for wear-aware purpose without the descend of allocation performance.

\subsection{Design Principles}

The most important design goal of wamalloc is wear-leveling. 
Hence, every time an application makes a memory request to gain a writeable address, 
a memory block with the least allocation time is expected to return.
However, a strict policy that returns the exact block with the least allocation time may introduce extra time penalty.
ln the later section we will see a hybrid method to tackle this problem.

Another requirement of wamalloc is low latency,
otherwise the memory would probably become the bottleneck of the entire application,
which results in allocator to be useless. 
To reduce the allocation latency, a local heap should be used by every thread to reduce the possible lock contention,
and all memory managed by a local heap should be allocated from a global heap.
however, using lock when request/free memory from global heap is inevitable,
as we will see later, we use some specific methods to reduce the use of lock.

\subsection{Overview Structure}

In this section, we will dig deeper into the design architecture of wamalloc.
The structure of wamalloc is showed in Figure~\ref{fig:arch}.
wamalloc maintains a local heap for each thread, and a global heap for all threads.
The reason why we use such a design is from the obvious observation that if a request from a thread can be satisfied by its local heap,
the allocation latency will be very low since there is no lock contention which may degrade the performance.
However, there must be a global heap that manage all the memory that maintained by each thread,
therefore a lock must be involved. One of our implementation focus is emphasized on how to reduce the use of lock to the best of our ability.

\begin{figure}
\centering
\includegraphics[width=3.2in]{arch.png}
\caption{overall architecture}
\label{fig:arch}
\end{figure}

Each local heap contains several fix-sized memory chunks.
Each memory chunk is further divided into several memory blocks based on its size class.
Each thread maintains an array of in-use memory chunk, an array of list that contains a list of waiting chunk and a free list to hold empty chunk.
When a thread makes a memory request of some certain memory class and there is no corresponding in-use chunk to answer that request,
the chunk of the same size class in waiting list is checked to become an in-use chunk. 
If still no usable memory chunks can be found,
the free list in the local heap or global heap will be visited based on our wear-aware policy,
which will be discussed in the further subsection.

\subsection{Memory Chunk}

Memory chunk is the basic building block that would be transferred between global heap and local heap. 
A memory chunk consists of two parts: a small chunk header and a 64KB chunk body. 
Chunk body is divided into several memory block based on the size class,
obviously, with smaller size class comes more memory blocks. 
Chunk header stores some metadata such as remaining free blocks number, size class, chunk owner and so on.

Since a memory chunk body only contains 64KB memory, all the memory request smaller than 64KB is regarded as small request,
which is directly handled by local heap without additional locking in the global heap.
Small allocations are divided into several predefined class, just like tcmalloc does. 
A memory request is served by a memory chunk with the nearest size class. 
Wamalloc uses two sets of size class: fine-grained(16B, 24B, 32B,..., 256B), coarse-grained(384B, 512B,..., 32768B, 49152B, 65536B). 
For the memory request larger than 64KB, wamalloc will redirect the request to operating system via system call such as mmap.

% classes of allocatable objects
\subsubsection{Classes of allocatable blocks}
The chunk overview is shown in Figure~\ref{fig:chunk}.
The usable memory blocks in a memory chunk can be classified into two categories:
first-allocated blocks indicated by the free pointer, which have never been allocated since this chunk is assigned to its local heap,
and reusable blocks indicated by the free list, which have been allocated and freed.
All the blocks starting from the free pointer where first-allocated objects start or at the free list storing reusable objects are ready to be allocated to applications. 
How to organize the free list will be discussed in the following wear-leveling policy subsection.

\begin{figure}[h]
\centering
\includegraphics[width=3.2in]{chunk.png}
\caption{chunk structure}
\label{fig:chunk}
\end{figure}

% how to choose chunk size
\subsubsection{How to choose chunk size}
The size of memory chunk is very critical to the performance of wamalloc. 
If the chunk size is too small, the memory block will be used up very soon and frequently communicating with the global heap will greatly decrease the performance.
If the chunk size is too large, lots of memory blocks may remain clean or unused, which leads to a huge memory waste. 
A working method used by many other allocators is to choose chunks of small size to serve small blocks, and chunks of large size to serve large blocks.
The reason why this method is adopted is that there will always be avaliable usable memory blocks no matter what size class it belongs to.
The drawbacks are also obvious.
Since the memory chunk size is different, it increases the complexity of global heap to manage these chunks.
Consequently this will cause the extra time plenalty and the possible memory fragment, which may lead to more physical memory than an application realy needs.

Take above discussions into consideration, we use a uniform memory chunk size for all size class. 
In most cases, memory requests are small allocations, which are naturly guaranteed that there are several usable memory blocks. 
For a special case, if a memory request is 64KB, there is only one block in the entire chunk body, so the allocation policy just view this block as the last remaining block 
and return it to the application. 
After that if the application needs a new 64KB memory, a new chunk is allocated to serve this request.
Further, This design gives us several other benifits: 
\begin{enumerate}
    \item Since all memory chunks are of the same size, then an empty chunk can easily be transferred from one size class to another without any particular actions.
The only overhead is to modify the metadata of the chunk, which can be negligible when compared to the non-uniform chunk size schema. 
    \item The uniform memory chunk simplified the design of global heap, which results in a lower time latency when communicating with global heap.
\end{enumerate}

\subsection{Local Heap}

Local heap is maintained by each thread. 
It is set up when a thread makes the first memory request.
Metadatas, such as size class, are stored in local heap. 
Every chunk in the local heap can be classified into five categories: In-use, Waiting, Full, Not-available, Clean.
The logical view of local heap and its relationship with memory chunk is shown in Figure~\ref{fig:heap}.

\begin{figure}[h]
\centering
\includegraphics[width=3.2in]{heap.png}
\caption{heap structure}
\label{fig:heap}
\end{figure}

\textbf{In-use chunk}. 
When an application request a memory block, wamalloc will first find the appropriate size class, then use this class number as index to get the currently using memory chunk. 
All small memory request is served by in-use chunk.

\textbf{Waiting chunk}. 
Chunks in waiting list are called waiting chunks. 
Waiting chunks are the chunks in a partially allocated state. 
When application use up the whole memory blocks in the current chunk, chunk of the same class size is a candidate waiting to be chosen as a new in-use chunk.

\textbf{Full chunk}. 
After several memory requests to a particular size class, a memory chunk may be out of blocks, which is called a full chunk.
No more blocks can be allocated from a chunk in a Full state , thus the following request will cause the allocator to get usable chunk from waiting chunks.
When a block is freed, the chunk it belongs to is in a full state and this full chunk will be added to the waiting list.

\textbf{Not-available chunk}.
Consider all the things we discussed so far, a typical life cycle of a memory chunk could be first used as in-use chunk, then waiting chunk, and back and forth repeatedly. 
For the wear-aware purpose, that is not what we expect. 
So we add a wear-count variable to each memory chunk and define an allocation threshold. 
When a chunk is first allocated from global heap, its wear-count variable is set to zero.
Every time a block is allocated from a chunk, its wear-count variable increases by one. 
If the wear-count variable of a chunk reaches the predefined allocation threshold, it is no longer available for allocation, and its state becomes Not-available. 
When all the blocks in a Not-available chunk are freed, this chunk is returned to global heap for further elaborate wear-leveling policy, which will be discussed in the following subsection.

\textbf{Clean chunk}.
After all the blocks in a waiting chunk are freed, the waiting chunk can be used as a clean chunk without a specific size class.
Hence it should be added to a free chunk pool, becoming a candidate for in-use chunk.
The promotion strategy must be carefully designed to achieve a wear-leveling purpose.

Based on what has been discussed so far,
we can draw the state machine of the relationship between different kinds of chunk in Figure~\ref{fig:state}.

\begin{figure}[h]
\centering
\includegraphics[width=3.2in]{state.png}
\caption{The state machine of chunk state}
\label{fig:state}
\end{figure}

\subsection{Global Heap}

Global heap maintains the whole memory chunks obtained from operating system. 
All the chunks of each thread are allocated from global heap. 
The logic view of global heap is shown in Figure~\ref{fig:global}.

\begin{figure}[h]
\centering
\includegraphics[width=3.2in]{global.png}
\caption{Global heap }
\label{fig:global}
\end{figure}

\subsubsection{Classes of allocatable chunks}
From the figure we can conclude that wamalloc first requests a huge continuous raw memory and divide it into several chunks ready to be allocated to different threads.
All the chunks starting from the free pointer or at the free list are ready to be allocated, 
just like what we have discussed in block allocation, how to allocate these free chunks is the key to wear-aware strategy.
If a chunk is allocated from the free pointer, wamalloc just returns the pointer and then increases this pointer by the size of memory chunk, 
otherwise a chunk is allocated from the free list, a wear-leveling policy must be applied.

\subsubsection{Method of enlarging global heap memory}
Global heap is enlarged via system call(e.g., mmap) if there is no available memory chunk.
Since global heap is shared by all threads, a lock is certainly required, which means a performance descend when lock contention happens.
Thus there exists two different opinions on how to increase the memory space. 
One possible solution aims at economic usage of virtual memory, for example, increasing the space linearly;
while the other solution aims at reducing the lock contention if possible, for example, increasing the space exponentially. 
In wamaloc, we use the latter solution for two reasons: 
\begin{enumerate}
    \item With the popularization of 64-bit machine, the size of virtual memory far exceeds the real physical memory an application needs.
Hence, it is unimportant that an application may take too many virtual memory. In return we can decrease the potential lock contention by acquiring the lock as less as possible.
    \item Almost all of the modern CPUs support page table mechanism, 
        which means the physical memory is mapped to some virtual address only when an application really needs the physical memory. 
        There is no page mapping at the virtual memory that an application does not touch, indicating that no physical memory is wasted.
\end{enumerate}

\subsubsection{How to reap chunks of terminated threads}
After a thread is terminated, there may be several remaining chunks in thread's local heap. 
We need to handle these memory chunks properly.
A typical method used in general-purpose memory allocator is to push these chunks into a global stack(FILO queue)
for the reason that these chunks are probably just used by the terminated thread and they may be in CPU's L1/L2/L3 cache, which is a cache-friendly option to reuse the chunk already in cache.
However, it is not a possible option for non-volatile memory because wear-leveling must be taken into consideration.
If we continuously serve an application with the hottest memory chunk, memory ceil would be damaged quickly.
More wear-leveling policy will be discussed in the next subsection.

\subsection{Wear-leveling policy in different level}

In this subsection we will discuss the wear-leveling policy in different level of wamalloc in detail.

The overall purpose is quite simple: all memory blocks are expected to be allocate evenly.

Traditional method uses first-in-first-out(FIFO) allocation policy(such as queue) or LRU\cite{zhou2009durable}\cite{rodriguez2015write}.
Although the advantage of FIFO is simple and fast, it is not an accurate wear-aware policy.
LRU may be better than FIFO in wear-aware accuracy, but the complexity of maintaining a LRU queue is too high which means a performance drop will happen.

NVMalloc\cite{moraru2013consistent} proposes a method to tackle this problem.
It timestamps every available free memory block to make sure every block is allocated at least in some time interval. NVMalloc implementsit as a dont-allocate list and on every allocation and release, the allocator examines the block at the head of the queue.
If it has been in this list for at least time $T$(a predefined threshold), allocator will remove it from list and mark it as available.
Thus, strictly speaking, it is just a modified FIFO and its drawbacks discussed above still exist.
Further more, making a memory block not available for a time $T$ period will consume more memory spaces. 
It is obvious since when there is no available block in the interval of $T$, the allocator need to ask more memory from operating system for new coming requests.

In wamalloc, explicit wear-leveling policy should be applied in two different places. 
We propose a novel hybrid wear-leveling method to balance the tradeoff between wear-leveling accuracy and performace.

% wear policy
\subsubsection{In block level}
Our target is to allocate memory blocks as evenly as possible.
Thus it is obviously that wamalloc should allocate memory block starting from the free pointer first
because these memory blocks have never been allocated since this chunk is assigned to its local heap.
After a chunk runs out of its first-allocated blocks, wamalloc will search the free list of the chunk to check whether any reusable block is available.
When a thread frees a memory block, this block should be added to the free list of its belonging chunk.
Thus, we should apply wear-leveling policy to this free list.
In consideration of allocation latency, no elaborate mechanism is employed for the sake of performance.
We consider free list as a FIFO queue. Queue may not give an accurate allocation, but in return the allocation latency is small.

\subsubsection{In local heap}

When the current in-use chunk is out of blocks, 
a chunk should be chosen as a new in-use chunk based on the wear-leveling policy.
According to the state machine shown in the previous section,
there are two sources that an available usable chunk can come from:
one is from the waiting chunk list, the other is from the free list of clean chunk in local heap.

Since Clean chunks do not belong to any size class, they are more flexible than the chunks in waiting chunk list.
In other words, if we have some clean chunks in the free list of local heap, 
we do not need to interfere with the global heap
because these clean chunks could meet requests of any size class directly when in-use chunk is out of blocks.
Thus waiting chunk list will be checked first to find the chunk that could replace the current in-use chunk.

Now we will discuss how to organize the waiting chunk list and the clean chunk list.
The method used here should take two things into consideration:
performance and wear-leveling.
Due to the fine-grained wear-leveling policy that will be discussed below,
here we put an emphasis on the performance and provide a coarse-grained wear-leveling policy.
Thus we think FIFO is our best choice, which has a $O(1)$ time complexity and a nearly optimal wear-leveling behavior.

\subsubsection{In global heap}
It is a common case that global heap need to reap the memory chunks returned from local heap.
Hence there is also a free chunk list in global heap maintaining all usable chunks. 
Just like the policy adopted in block level, 
when a new thread request a new memory chunk as its in-use chunk,
wamalloc will first allocate memory chunk starting from the free pointer; 
if no available chunk is found, wamalloc will allocate chunk from free chunk list complied with the following wear-leveling policy.

% hybrid method, 异步四叉堆
The best wear-leveling policy should return the least allocated objects so far. 
To the best of our knowledge, the most suitable data structure is a priority queue using allocation time as its key,
and the time complexity of finding the mininum key is $O(1)$, which sounds perfect to our wear-leveling policy.
However, now that it is the best, it is strange that no wear-aware NVM allocators use it.
We think the main reason is that its time complexity of the push operation is too high, which is $log(n)$, if priority queue is implemented by heaps. 
It will cause an impressive latency when the push operation happens frequently.

In wamalloc, we use two optimizations make the operations of priority queue not to be the bottleneck:
1) A 4-ary heap is used to implement the priority queue since it has a more CPU-friendlier behavior than other implementations.
2) Chunks are not moved to global pool synchronously, instead, the real operation is taken under the ground asynchronously.

Thus we use our optimized priority queue as the wear-leveling policy used in free chunk list without performace drop.
% The tradeoff is that it may consume a slight more memory space, which is in an acceptable range 

There are two cases when a thread needs to give memory chunk back to the global heap: 
1) when a thread is terminated, all its memory chunks should be returned back.
2) As we have discussed above, the wear-leveling policy in block level is FIFO, which is not an accurate method.
However, we can achieve a nearly optimal policy using following strategy: 
we define a threshold value \textbf{S} and an allocation time variable \textbf{alloctime} per chunk. 
Everytime a block is allocated from a chunk, its per chunk variable \textbf{alloctime} increases by one.
If \textbf{alloctime} exceed the predefined threshold \textbf{S}, 
the chunk is no longer available for serving new coming request, and state of the chunk becomes to Not-available.
When all memory blocks in a Not-available chunk are freed by a thread, this chunk will be moved to the global heap.
Instead of directly inserting the chunk to the priority queue in the global heap, wamalloc will do it asynchronously.
Specificly, the chunk will be first inserted into a producer-consumer queue, which is a $O(1)$ operations;
a background thread will periodly check the producer-consumer queue to get a returned chunk and insert it to the priority queue.
Since the producer-consumer queue is the global variable shared by all threads, another lock is needed.
A problem may be raised when lock contention happen.
As the evalution section shows, it is not quite frequent that wamalloc inserts a chunk into the producer-consumer queue,
which means the possible lock contention penalty will be amortized so that it won't be the bottleneck of wamalloc.

\subsection{Data Placement In DRAM And NVM}

Metadata in wamalloc can be classified into two different categories: data description and space description.
In this section, we will disscuss to which category metadatas previously mentioned belong and how to place them in hybrid memory system.
In wamalloc, metadata includes chunk header, local heap and global heap.
Chunk header belongs to data description category, in which it manages the blocks information inside a chunk and some chunk-related data, such as remaining free block counts, size class of current chunk, chunk owner, chunk state, chunk wear-leveling count and so on.
While local heap and global heap belong to space description, it manages where allocator can find the appropriate available chunks.

In traditional memory allocators, metadata and data are coupled with each other very tightly.
When an allocation or a deallocation request comes, there will be plenty of small metadata writes of changing pointer to chain its previous block and next block in order to split large space or merge small continuous spaces into a large free space.
This access pattern will decrease the lifetime of NVM because of the endurance problem.
Hence it is not what we expect in NVM allocator.

In wamalloc, we keep all metadatas in DRAM except chunk header. 
The reason is that when a thread frees a memory block back to allocator, a memory pointer \textbf{ptr} is passed as argument to wamalloc and its chunk header is easily computed by \textbf{ptr},
otherwise another mechanism would be introduced to remember this relationship, which will certainly cause an extra time cost.
At the meantime, wamalloc keeps the writing frequency of chunk header as less as possibile. Figure~\ref{fig:globalview} show the global view of wamalloc.
\begin{figure}[ht!]
\centering
\includegraphics[width=3.2in]{globalview.png}
\caption{Global view}
\label{fig:globalview}
\end{figure}

\subsection{Allocation Algorithm}

Based on the discussion above, we summarize the allocation algorithm in the following pseudo-code.

\begin{algorithm}
\caption{Allocation Algorithm}\label{euclid}
\begin{algorithmic}[1]
\Procedure{wamalloc}{size}

\State $\textit{sc} \gets \text{sizeToClass(}\textit{size}\text{);}$

\If {$\text{isSmallClass}(sc)$}
    \State $\text{lh} \gets \text{CurrentLocalHeap();}$
    \State $\text{retry:}$
    \State $\text{ch} \gets \text{InUseChunk(}\textit{lh, sc}\text{);}$
    \State $\text{ptr} \gets \text{AllocFromChunk(}\textit{ch}\text{);}$

    \If {$\text{IsEmptyChunk(}\textit{ch}\text{)}$}
        \State $\text{StateOf(}\textit{ch}\text{)} \gets \text{FULL;}$
        \State $\text{FindNewChunkToReplaceInuse(}\textit{lh, sc}\text{);}$
        \If {$\text{IsNULL(}\textit{ptr}\text{)}$}
            \State \textbf{goto} \emph{retry};
        \EndIf
    \EndIf

    \State \text{IncreaseWearCount(}\textit{ch, 1}\text{);}
    \If {$\text{WearCount(}\textit{ch}\text{)} \ge \textit{WEAR\_LIMIT}}$
        \State $\text{StateOf(}\textit{ch}\text{)} \gets \text{NotAvailable;}$
        \State $\text{FindNewChunkToReplaceInuse(}\textit{lh, sc}\text{);}$
    \EndIf

\Else
    \State $\text{ptr} \gets \text{LargeAlloc(}\textit{size}\text{);}$
\EndIf
\Return $\text{ptr;}$

\EndProcedure
\end{algorithmic}
\end{algorithm}

Wamalloc will first compute the size class according to the size parameter, 
then use this value to find the appropriate in-use chunk.
If it is a small size request, allocation will happen directly in the in-use chunk mentioned above.
Three exceptions could occur during an allocation:
1) the first time a thread make a request on class $n$, no in-use chunk is available in the current local heap;
2) the in-use chunk is out of free blocks;
3) when the wear-leveling variable reaches the predefined threshold.
All these exceptions cause the same action: the current chunk(in the first situation, it is a NULL pointer) should be replaced by another usable chunk.
If it is a large size request, the request will be redirect to operating system.

The critical path in allocation algorithm is short, which guarantees the performance stability.

\subsection{Deallocation Algorithm}

we summarize the deallocation algorithm in Algorithm 2.

\begin{algorithm}
\caption{Deallocation Algorithm}\label{euclid}
\begin{algorithmic}[1]
\Procedure{wafree}{ptr}

\State $\text{ch} \gets \text{ExtractHeader(ptr);}$
\State $\text{lh} \gets \text{CurrentLocalHeap();}$

\If {$\text{IsLarge(ch)} $}
\State \text{FreeLarge(ch)}
\Else
\State \text{PutPtrToFreeBlockPool(ptr);}
\State $\text{state} \gets \text{StateOf(ch);}$

\If {$\text{state} = \text{INUSE}$}
    \State \text{pass;}
\ElsIf {$\text{state} = \text{FULL}$}
    \State \text{AddToWaitingChunks(ch, lh);}
\ElsIf {$\text{state} = \text{WAITING}$}
    \If {$\text{AllBlocksAreFreed(ch)}$} 
        \State \text{AddToLocalHeapFreeList(ch);}
    \EndIf
\ElsIf {$\text{state} = \text{NOTAVAILABLE}$}
    \If {$\text{AllBlocksAreFreed(ch)}$} 
        \State \text{ReturnToGlobalHeap(ch);}
    \EndIf
\Else
    \State \text{Error: Unknown state.}
\EndIf
\EndIf

\EndProcedure
\end{algorithmic}
\end{algorithm}

If it is a large memory deallocation, the memory will be returned to operating system directly via system call.
Otherwise it is a small memory deallocation, wamalloc will perform different actions based on the current chunk state.
Specificly, there are four cases:
(1) if the current chunk is in an in-use state, nothing is done;
(2) in a Full state, it will be added to the corresponding waiting list of local heap;
(3) in a Waiting state and the chunk becomes a clean chunk, it will be added to the free list of local heap;
(4) in a Not-available state and the chunk becomes a clean chunk, it will be returned to the free list of global heap.

\subsection{Application level wear-leveling}

In the previous subsections, we have discussed how to implement an efficient wear-aware NVM allocator.
However, doing wear-leveling only in allocator level is far from enough.
Even if wamalloc returns the least allocated memory block, we can not control the access pattern of an application.  
For example, consider if an application once gets a writable memory block from wamalloc,
its continuous writes to this specific memory address without freeing it to wamalloc will cause the damage of NVM before long.

We want to prevent it from happening or find some way to redirect these continuous writes.
It is well known that all problems in computer science can be solved by another level of indirection.
One possible solution is to apply log-structure like strategy\cite{rumble2014log} to NVM, 
which results in sequential write instead of random write.
Another solution aims at building an virtual address mapping table which stores the mapping relationship between the source virtual address and the redirected destination virtual address\cite{shao2012ptl}.

Wamalloc is a software level library, it should depends on the current operating system and hardware architecture as less as possible.
Hence in wamalloc, we adopt the second method of maintaining the mapping between the virtual address and its redirected virtual address,
which has almost no dependency on the current system.

More specially, we design a function as Table~\ref{tab:api} shows.

\begin{table}[h]
\centering
\caption{APIs of application wear-leveling}
\begin{tabular}{|c|c|} \hline
API&Description\\ \hline
    wa\_write(dst, src, n)& copy n bytes from src to dst.\\ \hline
\end{tabular}
\label{tab:api}
\end{table}

The \textbf{dst} parameter in \textbf{wa\_write} is the address first returnen by wamalloc when an application make an allocation request.
The underlying implementation behind \textbf{wa\_write} includes a hashtable, in which a virtual address and its redirected virtual address is stored.
At first, it is easy to understand that the initial value of a redirect virtual address is equal to its corresponding virtual address.
With the calling time of \textbf{wa\_write} increasing, the virtual address is ready to be redirected to another cold memory block.
We define an threshold variable \textbf{WRITE\_LIMIT}.
After the calling time of \textbf{wa\_write} exceeds \textbf{WRITE\_LIMIT}, 
a new memory block is allocated to replace the current redirected memory block, which is done simply by calling wamalloc.
In other words, \textbf{wa\_write} is just a wrapper of wamalloc with an efficient hashtable implementation and an threshold variable \textbf{WRITE\_LIMIT}.

\section{Evaluation}

In this section, we evaluate wamalloc in several benchmarks and compare it against the glibc allocator and NVMalloc\cite{moraru2013consistent}.
We analyze wear-leveling as well as physical memory consumption and the performance of wamalloc under uniform and random allocation workloads.

\subsection{Benchmark Setup}

All evaluations were performed on a linux machine with 8 Intel Xeon 2GHz processors and 8GB DRAM, running the Linux 3.11 kernel.
We use DRAM as NVM proxy. It is important note that the evaluation software for NVM is not necessay for our evaluations, for the reason that wamalloc focus on the wear-leveling of allocation and the allocator perforance, rather than the performance of different NVM.
Thus if the conclusion is true performed in DRAM, it is also true performed in NVM.
Wamalloc is implemented in about 800 lines of code in C.

We decided to evaluate wear-leveling against NVMalloc and allocation performance against NVMalloc, as well as glibc,
which is compared to provide a performance baseline. 
The only three non-volatile allocator implementations publicly availbale at the time of writing are NVMalloc, nvm\_malloc, pmemalloc.
Nvm\_malloc provided a recovery mechanism and used the Persistent Memory Emulation Platform(PMEP)\cite{dulloor2014system} to emulate the performance of emerging NVM technologies.
Since wamalloc uses DRAM as a proxy and do not support recovery, we exclude nvm\_malloc for fairness reasons.
Pmemalloc uses a simple first-fit algorithm and thus performs very poorly at runtime costs with the increasement of allocation.
However, NVMalloc focused largely on designing a wear-aware allocation algorithms to increase the lifetime of NVM.
It turns out that our assumption, evaluation setup and the targeted problem are closest to these of in NVMalloc, so we will treat NVMalloc as contender.

\subsection{wear-leveling}

First, we evaluate the ability of our allocator to avoid NVM wear-out using a simple test program under uniform and random allocation and deallocation operations(50\% each).
In NVMalloc, for every allocation, the program writes the allocated block once, then use Pin\cite{luk2005pin} to record store to memory and compared the total writes of each block.
In this method, the total writes depends on not only the allocator itself but alse the test program behavior,
which we think may not be necessary and even worse, not be accurate in evaluating the wear-leveling ability of memory allocator.
we think it is more accurate to evaluate the allocation frequency of each block to illustrate the ability of wear-leveling.
More specificly, we run test to get the total allocation memory address, which can be stored in a set \textbf{V}.
If one specific memory address are allocated twice by the allocator, the address is stored once in set \textbf{V}.
The more evenly the addresses in \textbf{V} distribute, the better the wear-leveling policy is.
Thus we use (total allocation time)/(sizeof \textbf{V}) as average allocation frequency to metric the wear-leveling performance.

Figure~\ref{fig:eval_1} and Figure~\ref{fit:eval_3} show the result of wear-leveling based on the metric described above with uniform allocation of 128 byte and random distribution allocation between 64 and 512 bytes.
Since Wamalloc use a more elaborate and accurate wear-leveling policy than NVMalloc,
it is expectded that wamalloc performs better than NVMalloc.

\begin{figure}[ht]
\centering
\includegraphics[width=3in]{eval_1.png}
\caption{uniform allocations(128byte)}
\label{fig:eval_1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=3in]{eval_3.png}
\caption{random allocations between 64 and 512 bytes}
\label{fig:eval_3}
\end{figure}

In Figure~\ref{fig:eval_1}, as the number of malloc increases,
the average allocation frequency per block of NVMalloc is about 2.
while the value of that in Wamalloc is about 1.98, 
indicating Wamalloc performs better than NVMalloc under the workloads of uniform allocation.
In Figure~\ref{fig:eval_3},
Wamalloc also performs better than NVMalloc under the workload of random allocation.
This is explained by using the 4-ary heap implementing an accurate and elaborate wear-leveling policy.

However, it is meaningless to compare the average allocation frequency without comparing the physical memory consumption.
A simple allocator that allocate every memory block just once and never use that memory again, could make the average allocation frequency per block to be 1, while at the meantime it consume plenty of physical memory, causing it to be a useless allocator.
Thus, next we need also measure the memory consumption of above evaluations under uniform and random allocation.

Figure~\ref{fig:eval_2} and Figure~\ref{fig:eval_4} show the total memory consumption under the same workloads in Figure~\ref{fig:eval_1} and Figure~\ref{fig:eval_3}.
We can conclude from these figures that 
Wamalloc is much more economic in physical memory usage than NVMalloc.
Under uniform allocation, NVMalloc consumes nearly twice more memory than that Wamalloc consumes.
Under random allocation, the curve of memory consumption of NVMalloc and Wamalloc tend to be linear, 
however, Wamalloc still consumes less physical memory than NVMalloc by a constant difference.

\begin{figure}[ht]
\centering
\includegraphics[width=3in]{eval_2.png}
\caption{physical memory consumption under uniform allocations(128byte)}
\label{fig:eval_2}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=3in]{eval_4.png}
\caption{physical memory consumption under random allocations}
\label{fig:eval_4}
\end{figure}

In the above evalutions, we can arrive the conclusion that
Wamalloc perform better than NVMalloc not only in the wear-leveling policy,
but also in the physical memory consumptions.

\subsection{allocation performance}

Now, We are going to compare the allocation performance of each different allocator,
which is a crucial metric since it will affect the overall performance of a program.
Specificlly, we compare the average allocation time under workloads of different parrallel threads against NVMalloc.
We also add the gblic malloc(version 2.21) into comparison so that it can provide a performance baseline.

Unfortunately, NVMalloc does not support multithreading
so we modify the source code of NVMalloc to make multithreading supported in NVMalloc.

\subsection{Evalution Summary}

% ref nvm_malloc

\section{Related work}

TODO.

\section{Conclusions}
This paragraph will end the body of this sample document.
Remember that you might still have Acknowledgments or
Appendices; brief samples of these
follow.  There is still the Bibliography to deal with; and
we will make a disclaimer about that here: with the exception
of the reference to the \LaTeX\ book, the citations in
this paper are to articles which have nothing to
do with the present subject and are used as
examples only.
%\end{document}  % This is where a 'short' article might terminate

% ensure same length columns on last page (might need two sub-sequent latex runs)
\balance

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
This section is optional; it is a location for you
to acknowledge grants, funding, editing assistance and
what have you.  In the present case, for example, the
authors would like to thank Gerald Murray of ACM for
his help in codifying this \textit{Author's Guide}
and the \textbf{.cls} and \textbf{.tex} files that it describes.


% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{vldb_paper}  % vldb_sample.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references

%APPENDIX is optional.
% ****************** APPENDIX **************************************
% Example of an appendix; typically would start on a new page
%pagebreak

\begin{appendix}
You can use an appendix for optional proofs or details of your evaluation which are not absolutely necessary to the core understanding of your paper. 

\section{Final Thoughts on Good Layout}
Please use readable font sizes in the figures and graphs. Avoid tempering with the correct border values, and the spacing (and format) of both text and captions of the PVLDB format (e.g. captions are bold).

At the end, please check for an overall pleasant layout, e.g. by ensuring a readable and logical positioning of any floating figures and tables. Please also check for any line overflows, which are only allowed in extraordinary circumstances (such as wide formulas or URLs where a line wrap would be counterintuitive).

Use the \texttt{balance} package together with a \texttt{\char'134 balance} command at the end of your document to ensure that the last page has balanced (i.e. same length) columns.

\end{appendix}

\end{document}
